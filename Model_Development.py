# -*- coding: utf-8 -*-
"""Model_Development.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/bachvu98/Policy-NLP/blob/master/Model_Development.ipynb
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import string
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer
from os import listdir
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.model_selection import train_test_split, KFold
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix
from lxml import etree, html
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, log_loss
from sklearn.model_selection import GridSearchCV
nltk.download('stopwords')
nltk.download('punkt')

segments = pd.read_csv('https://raw.githubusercontent.com/bachvu98/Policy-NLP/master/OPP-115_v1_0/OPP-115/segment_processed.csv',index_col=0)
segments.head()

# create the object of tfid vectorizer
tfid_vectorizer = TfidfVectorizer("english")
# fit the vectorizer using the text data
tfid = tfid_vectorizer.fit(segments['segments'])
# collect the vocabulary items used in the vectorizer
dictionary = tfid_vectorizer.vocabulary_.items()

tfid_matrix = tfid_vectorizer.transform(segments['segments']).todense()
feature_names = tfid_vectorizer.get_feature_names()

# store the tf-idf array into pandas dataframe
df = pd.DataFrame(tfid_matrix, columns=feature_names)
df.head(10)

df['output'] = segments['category_name']
df['id'] = segments.index.get_level_values(0)
df.head(10)

features = df.columns.tolist()
output = 'output'
# removing the output and the id from features
features.remove(output)
features.remove('id')

alpha_list1 = np.linspace(0.005, 0.1, 20)
alpha_list1 = np.around(alpha_list1, decimals=4)
alpha_list1

# parameter grid
parameter_grid = [{"alpha":alpha_list1}]

parameter_grid

# classifier object
classifier1 = MultinomialNB()
# gridsearch object using 4 fold cross validation and neg_log_loss as scoring paramter
gridsearch1 = GridSearchCV(classifier1,parameter_grid, scoring = 'neg_log_loss', cv = 4)
# fit the gridsearch
gridsearch1.fit(df[features], df[output])

results1 = pd.DataFrame()
# collect alpha list
results1['alpha'] = gridsearch1.cv_results_['param_alpha'].data
# collect test scores
results1['neglogloss'] = gridsearch1.cv_results_['mean_test_score'].data

plt.rcParams['figure.figsize'] = (12.0, 6.0)
plt.plot(results1['alpha'], -results1['neglogloss'])
plt.xlabel('alpha')
plt.ylabel('logloss')
plt.grid()

print("Best parameter: ",gridsearch1.best_params_)

print("Best score: ",gridsearch1.best_score_)

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import KFold, cross_val_score

#Split data using 30%/70% split
segments_train, segments_test, targets_train, targets_test = train_test_split(segments['segments'], 
    segments['category_name'], test_size=0.3, random_state=50)

from sklearn.linear_model import SGDClassifier
from sklearn import svm
import time

#Set up an sklearn pipeline that processes policies, transforms them into a BOW model, applies TFIDF metric,
#then develops a Naive Bayes classifier.
text_clf = Pipeline([('vectorizer',CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', SGDClassifier())
])

alpha_list = np.linspace(0.001, 0.1, 100)
alpha_list

#paramater selection
start_time = time.time()
parameters = {'vectorizer__ngram_range': [(1, 1), (1, 2),(2,2)],
               'tfidf__use_idf': (True, False),
               'clf__alpha': alpha_list,}
gs_clf = GridSearchCV(text_clf, parameters, n_jobs = -1, cv = 5)
gs_clf = gs_clf.fit(segments_train, targets_train)
train_time = time.time() - start_time
print('Done training in', train_time, 'seconds.')

results = pd.DataFrame()
# collect alpha list
results['alpha'] = gs_clf.cv_results_['param_clf__alpha'].data
# collect test scores
results['score'] = gs_clf.cv_results_['mean_test_score'].data

plt.rcParams['figure.figsize'] = (12.0, 6.0)
plt.plot(results['alpha'], results['score'])
plt.xlabel('alpha')
plt.ylabel('score')
plt.grid()

print(gs_clf.best_score_)
print(gs_clf.best_params_)

text_clf_final = Pipeline([('vectorizer',CountVectorizer(ngram_range=(1,2))),
                     ('tfidf', TfidfTransformer(use_idf=True)),
                     ('clf', SGDClassifier(alpha=0.001))
])

text_clf_final.fit(segments_train,targets_train)
preds = text_clf_final.predict(segments_test)
print('Accuracy =', np.mean(preds == targets_test))

#Quick diagnostics
print(classification_report(targets_test.astype(str),preds.astype(str)))